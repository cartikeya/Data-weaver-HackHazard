# Data-weaver-HackHazard


![github-submission-banner](https://github.com/user-attachments/assets/a1493b84-e4e2-456e-a791-ce35ee2bcf2f)

# 🚀 Project Title

> DataWeave is an AI-powered web scraping and data processing tool that lets users extract information from any website, format it into CSV/JSON, and query it using multiple LLMs like Groq and GPT. It supports both direct URL scraping and prompt-based intelligent search, giving users full control over how data is gathered, cleaned, and analyzed—without writing a single line of code.

---

## 📌 Problem Statement

Select the problem statement number and title from the official list given in Participant Manual.

**Example:**  
**Problem Statement 1 – Weave AI Magic with groq**

---

## 🎯 Objective

DataWeave simplifies web data extraction using AI. It helps users quickly scrape, summarize, and structure data from websites, prompts, images, or audio — all powered by Groq.
Built for analysts, developers, and researchers, it turns unstructured content into usable insights and datasets within seconds.

---

## 🧠 Team & Approach

### Team Name:  
`scrape squad`

### Team Members:  
- Bhavesh - Full Stack & AI Integration
- Akshita - Frontend Developer (React, Tailwind)
- Cartikeya – Python Developer & UI Integration
- Mythilli – ML & Backend Logic


### Your Approach:  
- We chose this problem because web data is powerful but often messy and hard to access. We wanted to make scraping and dataset creation as easy as typing a prompt.
- Major challenges included handling dynamic content scraping, integrating Groq’s multimodal API, and converting user input into structured datasets.
- During development, we pivoted from a basic scraper to a full dataset builder — including text, image, and audio handling. A key breakthrough was successfully combining prompt-based scraping with Groq for rich, meaningful results.

---

## 🛠️ Tech Stack

### Core Technologies Used:
- Frontend: HTML, CSS, JavaScript, EJS, Tailwind CSS
- Backend: Node.js, Express.js
- Database: None (uses file system and in-memory storage)
- APIs: Groq LLMs, Puppeteer, Python subprocess (image scraping).

### Sponsor Technologies Used :
- ✅ **Groq:** _Used Groq’s ultra-fast LLMs to process and summarize scraped text, generate insights from user prompts, and convert multimedia content (text, image, audio) into structured datasets in real time_  
---

## ✨ Key Features

Highlight the most important features of your project:

- ✅ Prompt-Based Scraping: Users can simply type a prompt (e.g., “MS Dhoni retirement news”) and get summarized results using Groq.
- ✅ Multimodal Dataset Builder: Supports building datasets from text, image, and audio inputs — processed into downloadable formats.
- ✅ Live Web Scraping via URL: Extract content from any URL with dynamic control using a slider for content depth.
- ✅ JSON & ZIP Export Options: One-click export of scraped or generated data into .json or zipped .txt datasets.
- ✅ Mobile-Friendly UI: Built to be responsive and accessible across devices for quick testing.

Add images, GIFs, or screenshots if helpful!

---

## 📽️ Demo & Deliverables

- **Demo Video Link:** [Paste YouTube or Loom link here]  

---

## ✅ Tasks & Bonus Checklist

- ✅ **All members of the team completed the mandatory task - Followed at least 2 of our social channels and filled the form** (Details in Participant Manual)  
- ✅ **All members of the team completed Bonus Task 1 - Sharing of Badges and filled the form (2 points)**  (Details in Participant Manual)
- ✅ **All members of the team completed Bonus Task 2 - Signing up for Sprint.dev and filled the form (3 points)**  (Details in Participant Manual)

---

## 🧪 How to Run the Project

### Requirements:
-	Node.js (v16 or above)
-	Python 3.10+
-	Groq API Key (you’ll need to create a .env file)
-	Git (to clone the repo)

### Local Setup:
```bash

# 1. Clone the repository
git clone https://github.com/cartikeya/Data-weaver-HackHazard.git
cd Data-weaver-HackHazard

# 2. Install Node dependencies
npm install

# 3. Install Python dependencies
pip install -r requirements.txt

# 4. Add your API key
touch .env
# Paste this inside .env:
# GROQ_API_KEY=your_key_here

# 5. Run the project
node app.js
```

---

## 🧬 Future Scope



- 🔗 Multisite Scraping – Add support to scrape multiple websites in parallel for broader data extraction.
- 🧠 Custom AI Models – Let users plug in their own Groq API keys or models for tailored insights.
- 💾 Database Integration – Store user-generated datasets for history, reuse, or analytics.
- 🌐 Export Formats – Add support for exporting in CSV, XLSX, or custom API endpoints.
- 📱 Progressive Web App (PWA) – Turn the platform into a mobile-friendly offline-first tool.
- 🌍 Multilingual Support – Enable scraping and summarization in multiple languages.
- 🛡️ Authentication & Privacy – Add user logins, encryption, and file auto-deletion policies.

---

## 📎 Resources / Credits

- Groq LLMs – Used for ultra-fast text summarization and dataset generation.
- Puppeteer – Headless browser automation for URL-based scraping.
- icrawler – For scraping images from Google.
- BeautifulSoup4 – HTML parsing for web content.
- python-docx – Used to extract and process .docx files.
- Adm-Zip – Node.js package to zip scraped data.


⸻

## 🙌 Acknowledgements:
- Hackathon mentors and organizers for guidance and challenge framing.
- Open-source contributors for maintaining the packages used in this project.

---

## 🏁 Final Words

Akshita-
Bhavesh-
Mythili-
cartikeya-

---
